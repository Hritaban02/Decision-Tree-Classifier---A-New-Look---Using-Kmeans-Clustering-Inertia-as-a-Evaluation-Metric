{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Decision Tree Classifier - A New Evaluation Criterion using KMeans Clustering**","metadata":{"id":"LktTlT0CF-UL"}},{"cell_type":"markdown","source":"In this IPython Notebook, a decision tree classifier is implemented and tested on the [Avila dataset](https://archive.ics.uci.edu/ml/datasets/Avila) obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). BUT, a ***completely different evaluation criterion*** is used at each non-leaf node of the decision tree when choosing a decision attribute at that node.\n\nThere are a few **key** differences from a traditional decision tree.\n\n## Creation of the Node\n\nAt each node, \n*   First, check the number of unique classes present in the data that is supplied to that node (the whole dataset if the node is root or a subset of the data after splitting the parent node).\n    *   If the number of unique classes is 1, then the node is pure and is a leaf node. The class that will be classified by this leaf node will be that single unique class.\n    *   Else if the number of unique classes is more than 1, then the node is impure and can be split. The class that will be classified by this non-leaf node will be the class with the maximum frequency (this information is required in case the node is pruned later and is made a leaf node).\n        *    Let the number of unique classes be n where n > 1\n        *    Additionally, there is another parameter δ which denotes the ***number of decision attributes*** to be used at any node. This is an important key difference between this decision tree and traditional decision trees. In a traditional decision tree only one attribute could be used as a deciding attribute at any node, but in this decision tree which uses a new evaluation metric, any number of attributes can be used to collectively make a decision. This ***increases the power of the decision tree***.  \n        *    The new evaluation metric is nothing but the ***inertia*** of the KMeans Clustering.\n\n```\n# This is a pseudo-code to explain the working of this evaluation metric\nfor all possible combinations of attributes of the data of size δ:\n    # This means that if the data has attributes say x, y, z and δ = 3=2,\n    # then all possible combinations of attributes of size 2 would be xy, yz, and zx.\n\n    Perform KMeans Clustering on the data using a combination of attributes with n being the number of clusters.\n\n    # From the clustering model we can obtain the inertia and cluster centres.\n    Keep track of the combination of attributes and the corresponding KMeans clustering model for which the inertia is the minimum.\n\n# Now we have obtained a clustering model for a combination of attributes for which the inertia is minimum.\nUsing this model obtain the cluster centres and the corresponding data points which belong to each cluster centre.\nStore this information with the node (to be used later when the node is split).\n```\n\n## Splitting of the Node\n\nAfter the node is constructed, as usual if the node is impure it has to be split. BUT, in this decision tree the splitting is done based on the clusters obtained when the node was formed.\n\nWe split the impure node into n(number of clusters, also the number of unique classes at the node) nodes. Each child node is sent the data points corresponding a particular cluster centre and then the procedure is repeated for all the children. This constructs the decision tree. \n\n## Tree traversal for Decision Making\n\nOnce the tree is constructed, the traversal is very simple.\n\nTo classify a instance of the data say K,\n\n1.   Starting at the root, look at the children of the root. We have already stored the decision attributes combination to be used at the root and the cluster centres corresponding to each child.\n2.   Using the values of decision attributes combination in I, check which cluster centre is closest to K.\n3.   Then the data point K will be sent to that child whose cluster centre is closest to K. Now go back to Step 1 and repeat.\n4.   This process is repeated until a leaf is reached and then K is said to belong to the class corresponding to that leaf.","metadata":{"id":"RF-d74iKGZ3d"}},{"cell_type":"markdown","source":"# Import the required libraries","metadata":{"id":"xdpK9vKe5UxB"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport random\nimport pickle\nimport graphviz\nimport itertools\nimport threading\nfrom operator                   import itemgetter\nfrom graphviz                   import Digraph\nfrom sklearn.metrics            import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection    import train_test_split","metadata":{"id":"kClxJ4cUyIQa","execution":{"iopub.status.busy":"2022-05-07T17:47:29.948760Z","iopub.execute_input":"2022-05-07T17:47:29.949115Z","iopub.status.idle":"2022-05-07T17:47:30.354318Z","shell.execute_reply.started":"2022-05-07T17:47:29.949021Z","shell.execute_reply":"2022-05-07T17:47:30.353215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# KMeans Clustering","metadata":{"id":"JXGSAefY-0Wr"}},{"cell_type":"code","source":"class KMeans:\n    @staticmethod\n    def closest_centre(x, current_cluster_centres, distance_metric):\n        distance_to_each_centre = []\n        for centre in current_cluster_centres:\n            distance = np.linalg.norm(x - centre, distance_metric)\n            distance_to_each_centre.append(distance)\n        return np.where(distance_to_each_centre == np.amin(distance_to_each_centre))[0][0], np.square(np.amin(distance_to_each_centre))\n\n    def __init__(self, n_clusters=8, max_iter=300, distance_metric=None, algorithm='lloyd', initial_cluster_centres=None):\n        if algorithm != \"lloyd\" and algorithm != \"kmeans++\":\n            print(\"\\n Warning: Permissible methods of initialization are 'lloyd' and 'kmeans++'. algorithm value ignored and set to \\'kmeans++\\'.\")\n            algorithm = \"kmeans++\"\n        if initial_cluster_centres is not None and len(initial_cluster_centres) != self.n_clusters:\n            print(\"\\n Warning: Number of centres in initial_cluster_centres is not equal to n_clusters. initial_cluster_centres ignored and set to None.\")\n            initial_cluster_centres = None\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.distance_metric = distance_metric\n        self.algorithm = algorithm\n        self.initial_cluster_centres_ = initial_cluster_centres\n        self.cluster_centres_ = None\n        self.labels_ = None\n        self.inertia_ = None\n\n    def lloyd_cluster_centre_initialization(self, X):\n        return X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]\n\n    def kmeans_plus_plus_cluster_centre_initialization(self, X):\n        first_random_choice_for_centre = np.random.choice(X.shape[0], 1, replace=False)\n        current_cluster_centres = X[first_random_choice_for_centre]\n        for i in range(2, self.n_clusters + 1):\n            sum_of_square_of_min_distances = 0\n            probability_of_x_being_a_centre = []\n            for x in X:\n                _, distance_squared = self.closest_centre(x, current_cluster_centres, self.distance_metric)\n                probability_of_x_being_a_centre.append(distance_squared)\n                sum_of_square_of_min_distances += distance_squared\n            sum_of_square_of_min_distances += 0.000001\n            probability_of_x_being_a_centre = np.true_divide(np.array(probability_of_x_being_a_centre, dtype=float), sum_of_square_of_min_distances)\n            current_cluster_centres = np.append(current_cluster_centres, [X[np.where(probability_of_x_being_a_centre == np.amax(probability_of_x_being_a_centre))[0][0]]], axis=0)\n        return current_cluster_centres\n\n    def fit(self, X):\n        current_cluster_centres = None\n        inertia = None\n        labels = None\n        if self.initial_cluster_centres_ is None:\n            if self.algorithm == 'kmeans++':\n                current_cluster_centres = np.array(self.kmeans_plus_plus_cluster_centre_initialization(X), dtype=float)\n            elif self.algorithm == 'lloyd':\n                current_cluster_centres = np.array(self.lloyd_cluster_centre_initialization(X), dtype=float)\n        else:\n            current_cluster_centres = np.array(self.initial_cluster_centres_, dtype=float)\n        iterator = 1\n        while iterator <= self.max_iter:\n            recomputed_cluster_centres = {}\n            number_of_data_points_per_cluster = {}\n            inertia = 0.0\n            labels = []\n            for x in X:\n                label_of_x, distance_squared = self.closest_centre(x, current_cluster_centres, self.distance_metric)\n                labels.append(label_of_x)\n                inertia += distance_squared\n                if label_of_x in recomputed_cluster_centres:\n                    recomputed_cluster_centres[label_of_x] = np.add(recomputed_cluster_centres[label_of_x], x)\n                    number_of_data_points_per_cluster[label_of_x] += 1\n                else:\n                    recomputed_cluster_centres[label_of_x] = x\n                    number_of_data_points_per_cluster[label_of_x] = 1\n            for label in recomputed_cluster_centres:\n                recomputed_cluster_centres[label] = np.true_divide(recomputed_cluster_centres[label], number_of_data_points_per_cluster[label])\n            flag = True\n            for i in range(0, self.n_clusters):\n                if i in recomputed_cluster_centres and not np.array_equal(current_cluster_centres[i], recomputed_cluster_centres[i]):\n                    flag = False\n                if i in recomputed_cluster_centres:\n                    current_cluster_centres[i] = np.array(copy.deepcopy(recomputed_cluster_centres[i]), dtype=float)\n            if flag:\n                break\n            iterator += 1\n        self.cluster_centres_ = np.array(copy.deepcopy(current_cluster_centres), dtype=float)\n        self.inertia_ = inertia\n        self.labels_ = np.array(labels)\n        \n    def predict(self, X):\n        labels = []\n        for x in X:\n            label_of_x, _ = self.closest_centre(x, self.cluster_centres_, self.distance_metric)\n            labels.append(label_of_x)\n        return labels\n\n    def fit_predict(self, X):\n        self.fit(X)\n        return self.labels_","metadata":{"id":"pMmY5KmP8UWs","execution":{"iopub.status.busy":"2022-05-07T17:47:33.095975Z","iopub.execute_input":"2022-05-07T17:47:33.096744Z","iopub.status.idle":"2022-05-07T17:47:33.128822Z","shell.execute_reply.started":"2022-05-07T17:47:33.096701Z","shell.execute_reply":"2022-05-07T17:47:33.127329Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Node of the Decision Tree","metadata":{"id":"Ui7ULkLh-4yD"}},{"cell_type":"code","source":"class Node:\n    @staticmethod\n    def frequency(x):\n        return np.unique(np.array(x), return_counts=True)\n\n    def __init__(self, tree, x, y, attributes_left, column_names, parent_depth, max_depth=500, number_of_decision_attributes=1, distance_metric=None, clustering_algorithm='lloyd', clustering_max_iter=300):\n        tree.node_id_lock.acquire()\n        self.id = tree.node_id\n        tree.node_id += 1\n        tree.node_id_lock.release()\n        self.attributes_left = attributes_left\n        self.depth = parent_depth + 1\n        self.distance_metric = distance_metric\n        self.is_leaf = False\n        self.decision_attributes = None\n        self.decision_attributes_names = None\n        self.cluster_centers_cum_child = None\n        self.indices_list = None\n        self.inertia = None\n        rows, columns = x.shape\n        y_vals, frequencies = self.frequency(y)\n        self.vote = y_vals[np.argmax(frequencies)]\n        if self.depth < max_depth and len(y_vals) > 1 and len(self.attributes_left) > 0 and rows > 0:\n            _n_clusters = len(y_vals)\n            _minimum_within_cluster_sum_of_square = None\n            _decision_attributes = None\n            _clustering_model = None\n            _cluster_center_cum_child = []\n            for _decision_attributes_combination in list(itertools.combinations(attributes_left, number_of_decision_attributes)):\n                _decision_attributes_combination = list(_decision_attributes_combination)\n                clustering_model = KMeans(n_clusters=_n_clusters, max_iter=clustering_max_iter, distance_metric=self.distance_metric, algorithm=clustering_algorithm)\n                clustering_model.fit(x[:, _decision_attributes_combination])\n                if _minimum_within_cluster_sum_of_square is None or clustering_model.inertia_ < _minimum_within_cluster_sum_of_square:\n                    _minimum_within_cluster_sum_of_square = clustering_model.inertia_\n                    _decision_attributes = _decision_attributes_combination\n                    _clustering_model = copy.deepcopy(clustering_model)\n            self.inertia = _minimum_within_cluster_sum_of_square\n            self.decision_attributes = _decision_attributes\n            self.decision_attributes_names = itemgetter(*(self.decision_attributes))(column_names)\n            for _cluster_center in _clustering_model.cluster_centres_:\n                _cluster_center_cum_child.append([_cluster_center, ])\n            self.cluster_centers_cum_child = copy.deepcopy(_cluster_center_cum_child)\n            self.indices_list = []\n            _count_of_children = 0\n            for i in range(0, _n_clusters):\n                _index_list_for_ith_cluster = x[_clustering_model.labels_ == i, 0].astype(dtype=np.int64)\n                if len(_index_list_for_ith_cluster) == 0:\n                    self.cluster_centers_cum_child[i] = [None, None]\n                    self.indices_list.append(None)\n                else:\n                    self.indices_list.append(_index_list_for_ith_cluster)\n                    _count_of_children += 1\n            if _count_of_children == 1:\n                self.is_leaf = True\n        else:\n            self.is_leaf = True\n        # print(\"************************\")\n        # print(\"Node ID: \", self.id)\n        # print(\"Vote: \", self.vote)\n        # print(\"Is Leaf: \", self.is_leaf)\n        # print(\"Decision Attributes: \", self.decision_attributes_names)\n        # print(\"Cluster Center: \", self.cluster_centers_cum_child)\n        # print(\"Inertia: \", self.inertia)\n        # print(\"************************\")\n\n    def test(self, x, usable_depth):\n        if self.depth > usable_depth or self.is_leaf:\n            return self.vote\n        else:\n            new_x = x[self.decision_attributes]\n            minimum_distance = None\n            next_child = None\n            for center, child in self.cluster_centers_cum_child:\n                if center is not None and child is not None:\n                    distance = np.linalg.norm(new_x - center, ord=self.distance_metric)\n                    if minimum_distance is None or distance < minimum_distance:\n                        minimum_distance = distance\n                        next_child = child\n            return next_child.test(x, usable_depth)\n\n    def get_info(self):\n        if not self.is_leaf:\n            return \"node #{}, attribute={}, vote={}\".format(self.id, self.decision_attributes_names, self.vote)\n        return \"node #{}, vote={}\".format(self.id, self.vote)","metadata":{"id":"R2nzO2qJIg4m","execution":{"iopub.status.busy":"2022-05-07T17:47:36.311963Z","iopub.execute_input":"2022-05-07T17:47:36.312284Z","iopub.status.idle":"2022-05-07T17:47:36.342090Z","shell.execute_reply.started":"2022-05-07T17:47:36.312229Z","shell.execute_reply":"2022-05-07T17:47:36.340851Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{"id":"tDsedv26-9Q-"}},{"cell_type":"code","source":"class DecisionTreeClassifier:\n    def __init__(self, reuse_attribute=False, max_depth=100, number_of_decision_attributes=1, distance_metric=None, clustering_algorithm='lloyd', clustering_max_iter=300):\n        self.node_id = 1\n        self.node_id_lock = threading.Lock()\n        self.reuse_attribute = reuse_attribute\n        self.max_depth = max_depth\n        self.number_of_decision_attributes = number_of_decision_attributes\n        self.distance_metric = distance_metric\n        self.clustering_algorithm = clustering_algorithm\n        self.clustering_max_iter = clustering_max_iter\n        self.root = None\n        self.node_dict = {}\n        self.node_at_depth_count = {}\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, x, y, column_names):\n        self.X_train = x\n        self.y_train = y\n        rows, columns = self.X_train.shape\n        attributes_list = list(range(1, columns))\n        self.root = Node(self, x=self.X_train, y=self.y_train, attributes_left=attributes_list, column_names=column_names, parent_depth=0, max_depth=self.max_depth, number_of_decision_attributes=self.number_of_decision_attributes, distance_metric=self.distance_metric, clustering_algorithm=self.clustering_algorithm, clustering_max_iter=self.clustering_max_iter)\n        self.node_dict[self.root.id] = self.root\n        if '1' not in self.node_at_depth_count.keys():\n            self.node_at_depth_count['1'] = 1\n        else:\n            self.node_at_depth_count['1'] += 1\n        if not self.reuse_attribute:\n            for attribute in self.root.decision_attributes:\n                attributes_list.remove(attribute)\n        self.build(self.root, attributes_list)\n        self.node_id_lock = None\n\n    def build(self, root, attributes_left):\n        if not root.is_leaf:\n            for i in range(0, len(root.cluster_centers_cum_child)):\n                if root.indices_list[i] is not None:\n                    subset_mask = np.isin(self.X_train[:, 0].astype(np.int64), root.indices_list[i])\n                    child = Node(self, x=self.X_train[subset_mask, :], y=self.y_train[subset_mask], attributes_left=attributes_left, column_names=column_names, parent_depth=root.depth, max_depth=self.max_depth, number_of_decision_attributes=self.number_of_decision_attributes, distance_metric=self.distance_metric, clustering_algorithm=self.clustering_algorithm, clustering_max_iter=self.clustering_max_iter)\n                    root.cluster_centers_cum_child[i].append(child)\n                    self.node_dict[child.id] = child\n            if str(root.depth + 1) not in self.node_at_depth_count.keys():\n                self.node_at_depth_count[str(root.depth + 1)] = len(root.cluster_centers_cum_child)\n            else:\n                self.node_at_depth_count[str(root.depth + 1)] += len(root.cluster_centers_cum_child)\n            t = []\n            for i in range(0, len(root.cluster_centers_cum_child)):\n                attributes_list_copy = copy.deepcopy(attributes_left)\n                child = root.cluster_centers_cum_child[i][1]\n                if child is not None:\n                    if not child.is_leaf:\n                        if not self.reuse_attribute:\n                            for attribute in child.decision_attributes:\n                                attributes_list_copy.remove(attribute)\n                        t_ = threading.Thread(target=self.build, args=(child, attributes_list_copy,))\n\n                        t.append(t_)\n                        t_.start()\n            for t_ in t:\n                t_.join()\n\n    def predict_thread(self, x, y, use_depth, i):\n        y[i] = self.root.test(x, use_depth)\n\n    def predict(self, x, use_depth=100):\n        rows, _ = x.shape\n        y_predict = [0]*rows\n        t = []\n        for i in range(0, rows):\n            t_ = threading.Thread(target=self.predict_thread, args=(x[i, :], y_predict, use_depth, i,))\n            t.append(t_)\n            t_.start()\n        for t_ in t:\n            t_.join()\n        return y_predict\n\n    @staticmethod\n    def prune_thread(model, id, accuracy_dict, X_valid, y_valid, minimum_increase, accuracy_before_pruning):\n        model_temp = copy.deepcopy(model)\n        model_temp.node_dict[id].is_leaf = True\n        accuracy_after_pruning = accuracy_score(y_valid, model_temp.predict(X_valid))\n        if accuracy_after_pruning > accuracy_before_pruning + minimum_increase:\n            accuracy_dict[id] = accuracy_after_pruning\n        model_temp.node_dict[id].is_leaf = False\n\n    @staticmethod\n    def prune_thread_for_stopping_rounds(model, id, accuracy_dict, X_valid, y_valid):\n        model_temp = copy.deepcopy(model)\n        model_temp.node_dict[id].is_leaf = True\n        accuracy_dict[id] = accuracy_score(y_valid, model_temp.predict(X_valid))\n        model_temp.node_dict[id].is_leaf = False\n\n    def prune(self, evaluation_set, minimum_increase=0.001, maximum_rounds=10, sample_size=10, stopping_rounds=0):\n        (X_valid, y_valid) = evaluation_set\n        accuracy_before_pruning = accuracy_score(y_valid, self.predict(X_valid))\n        for i in range(0, maximum_rounds+1):\n            accuracy_dict = {}\n            sample_size = min(len(self.node_dict.items()), sample_size)\n            random_sample = dict(random.sample(self.node_dict.items(), sample_size))\n            t = []\n            print(\"************************\")\n            print(\"Pruning Step: \", i)\n            print(\"Number of nodes in the Tree: \", len(self.node_dict))\n            print(\"************************\")\n            for node_i in random_sample.values():\n                if not node_i.is_leaf:\n                    t_ = threading.Thread(target=self.prune_thread, args=(self, node_i.id, accuracy_dict, X_valid, y_valid, minimum_increase, accuracy_before_pruning,))\n                    t.append(t_)\n                    t_.start()\n            for t_ in t:\n                t_.join()\n            if len(accuracy_dict) == 0:\n                if sample_size >= len(self.node_dict):\n                    break\n                else:\n                    continue\n            max_accuracy_node_id = max(accuracy_dict, key=accuracy_dict.get)\n            pruned_node = self.node_dict[max_accuracy_node_id]\n            # print(\"************************\")\n            # print(\"Pruned Node with ID: \", pruned_node.id)\n            # print(\"Accuracy after pruning on Validation Set: \", accuracy_dict[max_accuracy_node_id]*100)\n            # print(\"Increase in accuracy: \", (accuracy_dict[max_accuracy_node_id]-accuracy_before_pruning)*100)\n            # print(\"************************\")\n            self.remove_children(pruned_node.id)\n            pruned_node.is_leaf = True\n            accuracy_before_pruning = accuracy_dict[max_accuracy_node_id]\n        if stopping_rounds > 0 and len(self.node_dict) > 0:\n            accuracy_before_last_round = accuracy_score(y_valid, self.predict(X_valid))\n            accuracy_after_last_round = accuracy_before_last_round\n            for i in range(0, stopping_rounds):\n                accuracy_dict = {}\n                sample_size = min(len(self.node_dict.items()), sample_size)\n                random_sample = dict(random.sample(self.node_dict.items(), sample_size))\n                t = []\n                for node_i in random_sample.values():\n                    if not node_i.is_leaf:\n                        t_ = threading.Thread(target=self.prune_thread_for_stopping_rounds, args=(self, node_i.id, accuracy_dict, X_valid, y_valid,))\n                        t.append(t_)\n                        t_.start()\n                for t_ in t:\n                    t_.join()\n                if len(accuracy_dict) == 0:\n                    continue\n                max_accuracy_node_id = max(accuracy_dict, key=accuracy_dict.get)\n                # print(\"************************\")\n                # print(\"Pruning On Stopping Rounds\")\n                # print(\"Pruned Node with ID: \", max_accuracy_node_id)\n                # print(\"Accuracy after pruning on Validation Set: \", accuracy_dict[max_accuracy_node_id]*100)\n                # print(\"Change in accuracy: \", (accuracy_dict[max_accuracy_node_id]-accuracy_before_last_round)*100)\n                # print(\"************************\")\n                if i < stopping_rounds-1:\n                    accuracy_before_last_round = accuracy_dict[max_accuracy_node_id]\n                accuracy_after_last_round = accuracy_dict[max_accuracy_node_id]\n                pruned_node = self.node_dict[max_accuracy_node_id]\n                self.remove_children(pruned_node.id)\n                pruned_node.is_leaf = True\n            if accuracy_after_last_round > accuracy_before_last_round + minimum_increase:\n                self.prune(evaluation_set, stopping_rounds=0)\n\n    @staticmethod\n    def prune_node_vary_thread(model, id, accuracy_dict, X_valid, y_valid):\n        model_temp = copy.deepcopy(model)\n        model_temp.node_dict[id].is_leaf = True\n        accuracy_dict[id] = accuracy_score(y_valid, model_temp.predict(X_valid))\n        model_temp.node_dict[id].is_leaf = False\n\n    def prune_node_vary(self, evaluation_set_train, evaluation_set_test, evaluation_set_val):\n        train_accuracies = []\n        test_accuracies = []\n        val_accuracies = []\n        prunes = []\n        (X_train, y_train) = evaluation_set_train\n        (X_test, y_test) = evaluation_set_test\n        (X_valid, y_valid) = evaluation_set_val\n        pruned = 0\n        pruned_flag = False\n        accuracy_before_pruning = accuracy_score(y_valid, self.predict(X_valid))\n        while len(self.node_dict) > 1:\n            train_acc = accuracy_score(y_train, self.predict(X_train))\n            test_acc = accuracy_score(y_test, self.predict(X_test))\n            train_accuracies.append(train_acc)\n            test_accuracies.append(test_acc)\n            val_accuracies.append(accuracy_before_pruning)\n            prunes.append(len(self.node_dict))\n            accuracy_dict = {}\n            t = []\n            print(\"************************\")\n            print(f\"Pruning Step : {pruned}\")\n            print(f\"Number of nodes in the tree: {len(self.node_dict.values())}\")\n            print(\"************************\")\n            for node_i in self.node_dict.values():\n                if not node_i.is_leaf:\n                    t_ = threading.Thread(target=self.prune_node_vary_thread, args=(self, node_i.id, accuracy_dict, X_valid, y_valid))\n                    t.append(t_)\n                    t_.start()\n            for t_ in t:\n                t_.join()\n            if len(accuracy_dict) == 0:\n                break\n            max_accuracy_node_id = max(accuracy_dict, key=accuracy_dict.get)\n            accuracy_after_pruning = accuracy_dict[max_accuracy_node_id]\n            pruned_node = self.node_dict[max_accuracy_node_id]\n            self.remove_children(pruned_node.id)\n            pruned_node.is_leaf = True\n            if not pruned_flag and accuracy_after_pruning < accuracy_before_pruning:\n                pickle.dump(self, open(\"fully_pruned_decision_tree.dat\", \"wb\"))\n                self.print_decision_tree(f\"fully_pruned_\")\n                pruned_flag = True\n            pickle.dump(self, open(f\"pruning_step_{pruned}_decision_tree.dat\", \"wb\"))\n            self.print_decision_tree(f\"pruning_step_{pruned}_\")\n            np.save(open(\"train_accuracies.npy\", \"wb\"), train_accuracies)\n            np.save(open(\"test_accuracies.npy\", \"wb\"), test_accuracies)\n            np.save(open(\"val_accuracies.npy\", \"wb\"), val_accuracies)\n            np.save(open(\"prunes.npy\", \"wb\"), prunes)\n            pruned = pruned + 1\n            accuracy_before_pruning = accuracy_dict[max_accuracy_node_id]\n        return train_accuracies, test_accuracies, val_accuracies, prunes\n\n    def remove_children(self, node_id):\n        if node_id not in self.node_dict or self.node_dict[node_id].is_leaf:          \n            return\n        elif self.node_dict[node_id].cluster_centers_cum_child is not None and len(self.node_dict[node_id].cluster_centers_cum_child) != 0:\n            for _, child in self.node_dict[node_id].cluster_centers_cum_child:\n                if child is not None:\n                    self.remove_children(child.id)\n                    if child.id in self.node_dict:\n                        self.node_dict.pop(child.id)\n\n    def print_decision_tree(self, filepath=\"\"):\n        f = Digraph('Decision Tree', filename=f'{filepath}decision_tree.gv')\n        f.attr(rankdir='LR', size='1000,500')\n        f.attr('node', shape='rectangle')\n        if self.root.is_leaf:\n            f.node(\"node #{} , vote={}\".format(self.root.id, self.root.vote))\n        else:\n            q = [self.root]\n            while len(q) > 0:\n                node = q.pop(0)\n                if not node.is_leaf:\n                    for i in range(0, len(node.cluster_centers_cum_child)):\n                        if node.cluster_centers_cum_child[i][0] is not None and node.cluster_centers_cum_child[i][1] is not None:\n                            f.edge(node.get_info(), node.cluster_centers_cum_child[i][1].get_info(), label='center=' + str(node.cluster_centers_cum_child[i][0]))\n                            q.append(node.cluster_centers_cum_child[i][1])\n        f.render(f'{filepath}decision_tree.gv', view=True)\n        with open(f\"{filepath}decision_tree.gv\") as f:\n            dot_graph = f.read()\n        graphviz.Source(dot_graph)\n        os.remove(f\"{filepath}decision_tree.gv\")","metadata":{"id":"lmMHY6wSx_Yk","execution":{"iopub.status.busy":"2022-05-07T17:47:40.208047Z","iopub.execute_input":"2022-05-07T17:47:40.208447Z","iopub.status.idle":"2022-05-07T17:47:40.293506Z","shell.execute_reply.started":"2022-05-07T17:47:40.208414Z","shell.execute_reply":"2022-05-07T17:47:40.292666Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Analysis PART - 1","metadata":{"id":"_wXRw3mE_AWq"}},{"cell_type":"code","source":"filepath = '../input/avila-dataset-and-saved-variables/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:47:46.426892Z","iopub.execute_input":"2022-05-07T17:47:46.427197Z","iopub.status.idle":"2022-05-07T17:47:46.431859Z","shell.execute_reply.started":"2022-05-07T17:47:46.427160Z","shell.execute_reply":"2022-05-07T17:47:46.430750Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(filepath+'avila_combined.txt', delimiter=\",\", header=None)\n\ncolumn_names = ['index', 'intercolumnar distance', 'upper margin', 'lower margin', 'exploitation', 'row number', 'modular ratio',\n              'interlinear spacing', 'weight', 'peak number', 'modular ratio/ interlinear spacing']\n\nX = df.iloc[:, 0:-1].to_numpy()\ny = df.iloc[:, -1].to_numpy()\nindex = np.arange(X.shape[0])\nX = np.c_[index, X]","metadata":{"id":"CWthiPUIv93H","execution":{"iopub.status.busy":"2022-05-07T17:47:47.888830Z","iopub.execute_input":"2022-05-07T17:47:47.889073Z","iopub.status.idle":"2022-05-07T17:47:47.924931Z","shell.execute_reply.started":"2022-05-07T17:47:47.889045Z","shell.execute_reply":"2022-05-07T17:47:47.924257Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def on_average_analysis_thread(X, y, i, column_names, output, N):\n    X_sub, X_test, y_sub, y_test = train_test_split(X, y, test_size=0.2, random_state=i*N + 1*N)\n    X_train, X_valid, y_train, y_valid = train_test_split(X_sub, y_sub, test_size=0.1, random_state=i*N + 1*N)\n    model = DecisionTreeClassifier(reuse_attribute=False, max_depth=100, number_of_decision_attributes=1, distance_metric=None, clustering_algorithm='kmeans++', clustering_max_iter=300)\n    model.fit(X_train, y_train, column_names)\n    y_predict = model.predict(X_test)\n    ac = accuracy_score(y_test, y_predict)\n    output[i] = (ac, model, X_test, y_test, X_valid, y_valid, X_train, y_train)\n\nN = 5\nt = []\noutput = [0]*N\nfor i in range(0, N):\n    t_ = threading.Thread(target=on_average_analysis_thread, args=(X, y, i, column_names, output, N, ))        \n    t.append(t_)\n    t_.start()\nfor t_ in t:\n    t_.join()\n\nfinal_model = None\nmax_test_accuracy = 0.0\nfinal_X_test = None\nfinal_y_test = None\nfinal_X_valid = None\nfinal_y_valid = None\nfinal_X_train = None\nfinal_y_train = None\naccuracy_sum = 0.0\nanalysisFile = open('analysis.txt', 'w')\nfor i in range(0, N):\n    ac, model, X_test, y_test, X_valid, y_valid, X_train, y_train = output[i]\n    print(\"***********************\", file=analysisFile)\n    print(f\"Random split #{i + 1} \", file=analysisFile)\n    print(f\"Accuracy = {ac * 100}%\", file=analysisFile)\n    print(\"***********************\", file=analysisFile)\n    accuracy_sum += ac\n    if ac > max_test_accuracy:\n        max_test_accuracy = ac\n        final_model = model\n        final_X_test = X_test\n        final_y_test = y_test\n        final_X_valid = X_valid\n        final_y_valid = y_valid\n        final_X_train = X_train\n        final_y_train = y_train\naccuracy_sum /= N    \n\nprint(\"Number of nodes in the tree: \", len(final_model.node_dict), file=analysisFile)\n\nfinal_model.print_decision_tree(f\"\")\n\ny_predict = final_model.predict(final_X_test)\nac = accuracy_score(final_y_test, y_predict)\nprint(f\"Maximum Accuracy = {ac * 100}%, average accuracy = {accuracy_sum * 100}%\", file=analysisFile)\n\nclasses = np.unique(final_y_test)\nmatrix = confusion_matrix(final_y_test, y_predict, labels=classes)\nprint('Confusion matrix : \\n', matrix, file=analysisFile)\n\nmatrix = classification_report(final_y_test, y_predict, labels=classes)\nprint('Classification report : \\n', matrix, file=analysisFile)\n\npickle.dump(final_model, open(\"final_model.dat\", \"wb\"))\nnp.save(open(filepath+\"final_X_train.npy\", \"wb\"), final_X_train)\nnp.save(open(filepath+\"final_y_train.npy\", \"wb\"), final_y_train)\nnp.save(open(filepath+\"final_X_test.npy\", \"wb\"), final_X_test)\nnp.save(open(filepath+\"final_y_test.npy\", \"wb\"), final_y_test)\nnp.save(open(filepath+\"final_X_valid.npy\", \"wb\"), final_X_valid)\nnp.save(open(filepath+\"final_y_valid.npy\", \"wb\"), final_y_valid)\nanalysisFile.close()","metadata":{"id":"Ce62PjRcn-tQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***********************\nRandom split #1 \nAccuracy = 86.15237182558697%\n***********************\n***********************\nRandom split #2 \nAccuracy = 89.57834211787255%\n***********************\n***********************\nRandom split #3 \nAccuracy = 87.2065165309056%\n***********************\n***********************\nRandom split #4 \nAccuracy = 88.83564925730714%\n***********************\n***********************\nRandom split #5 \nAccuracy = 90.08145663632008%\n***********************\n\n    Number of nodes in the tree:  3443\n\n    Maximum Accuracy = 90.08145663632008%, average accuracy = 88.37086727359846%\n\n    Confusion matrix : \n     \n     [[1594    0    1    0    7  110   26   15    0    1    0    0]\n     [   0    2    0    0    0    0    0    0    0    0    0    0]\n     [   1    0   33    0    0    0    0    1    0    0    0    0]\n     [   0    0    0  141    4    0    0    0    0    0    0    0]\n     [  11    0    0    9  417    4    1    1    0    0    8    0]\n     [ 117    0    0    0    2  630    4    4    0    0    0    0]\n     [  12    0    0    0    6   18  145    2    0    0    0    0]\n     [  17    0    0    0    3    8    3  182    0    0    0    0]\n     [   0    0    0    0    0    0    0    0  323    0    0    0]\n     [   0    0    0    0    0    0    0    0    0   15    0    0]\n     [   6    0    0    0    4    0    0    0    0    0  174    6]\n     [   0    0    0    0    0    0    0    0    0    0    2  104]]\n \n    Classification report : \n\n              precision    recall  f1-score   support\n               \n           A       0.91      0.91      0.91      1754\n           B       1.00      1.00      1.00         2\n           C       0.97      0.94      0.96        35\n           D       0.94      0.97      0.96       145\n           E       0.94      0.92      0.93       451\n           F       0.82      0.83      0.83       757\n           G       0.81      0.79      0.80       183\n           H       0.89      0.85      0.87       213\n           I       1.00      1.00      1.00       323\n           W       0.94      1.00      0.97        15\n           X       0.95      0.92      0.93       190\n           Y       0.95      0.98      0.96       106\n           \n        accuracy                           0.90      4174\n       macro avg       0.93      0.93      0.93      4174\n    weighted avg       0.90      0.90      0.90      4174","metadata":{}},{"cell_type":"markdown","source":"# Analysis for Depth of Tree","metadata":{"id":"WGLmGDoWZnWm"}},{"cell_type":"code","source":"train_accuracies = []\ntest_accuracies = []\nval_accuracies = []\ndepths = []\nnodes = []\n\nmax_depth = 10\nfor depth in range(1, max_depth+1):\n    y_predict = final_model.predict(final_X_test, use_depth=depth)\n    ac = accuracy_score(final_y_test, y_predict)\n    test_accuracies.append(ac)\n    y_predict = final_model.predict(final_X_train, use_depth=depth)\n    ac = accuracy_score(final_y_train, y_predict)\n    train_accuracies.append(ac)\n    y_predict = final_model.predict(final_X_valid, use_depth=depth)\n    ac = accuracy_score(final_y_valid, y_predict)\n    val_accuracies.append(ac)\n    depths.append(depth)\n    if not len(nodes):\n        nodes.append(final_model.node_at_depth_count[str(depth)])\n    else:\n        if str(depth) in final_model.node_at_depth_count.keys():\n            nodes.append(nodes[-1] + final_model.node_at_depth_count[str(depth)])\n        else:\n            nodes.append(nodes[-1])\n\nplt.plot(depths, train_accuracies, label=\"training accuracy\")\nplt.plot(depths, test_accuracies, label=\"test accuracy\")\nplt.plot(depths, val_accuracies, label=\"validation accuracy\")\nplt.legend()\nplt.xlabel('Depth of the Decision Tree')\nplt.ylabel('Accuracy')\nplt.title(\"Evaluation Criterion - KMeans Clustering\")\nplt.savefig(f'depth_analysis.png', dpi=300, bbox_inches='tight')\nplt.clf()","metadata":{"id":"dhMSIMAWZpJ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Depth Analysis Graph](https://github.com/Hritaban02/Decision-Tree-Classifier---A-New-Look---Using-Kmeans-Clustering-Inertia-as-a-Evaluation-Metric/blob/master/Analysis%20Part%201/depth_analysis.png)","metadata":{}},{"cell_type":"markdown","source":"# Analysis for the Number of Nodes in the Tree","metadata":{"id":"c5xjk6b8c_uT"}},{"cell_type":"code","source":"plt.plot(nodes, train_accuracies, label=\"training accuracy\")\nplt.plot(nodes, test_accuracies, label=\"test accuracy\")\nplt.plot(nodes, val_accuracies, label=\"validation accuracy\")\nplt.legend()\nplt.xlabel('Number of Nodes in the Decision Tree')\nplt.ylabel('Accuracy')\nplt.title(\"Evaluation Criterion - KMeans Clustering\")\nplt.savefig(f'node_analysis.png', dpi=300, bbox_inches='tight')\nplt.clf()","metadata":{"id":"CmeKZOt7dAcD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Node Analysis Graph](https://github.com/Hritaban02/Decision-Tree-Classifier---A-New-Look---Using-Kmeans-Clustering-Inertia-as-a-Evaluation-Metric/blob/master/Analysis%20Part%201/node_analysis.png)","metadata":{}},{"cell_type":"markdown","source":"# Analysis of the status of tree during pruning","metadata":{"id":"uBxpsGpadSaG"}},{"cell_type":"code","source":"final_model = pickle.load(open(filepath+\"final_model.dat\", \"rb\"))\nfinal_X_train = np.load(open(filepath+\"final_X_train.npy\", \"rb\"), allow_pickle=True)\nfinal_y_train = np.load(open(filepath+\"final_y_train.npy\", \"rb\"), allow_pickle=True)\nfinal_X_test = np.load(open(filepath+\"final_X_test.npy\", \"rb\"), allow_pickle=True)\nfinal_y_test = np.load(open(filepath+\"final_y_test.npy\", \"rb\"), allow_pickle=True)\nfinal_X_valid = np.load(open(filepath+\"final_X_valid.npy\", \"rb\"), allow_pickle=True)\nfinal_y_valid = np.load(open(filepath+\"final_y_valid.npy\", \"rb\"), allow_pickle=True)\n\nmodel_temp = copy.deepcopy(final_model)\na, b, c, d = model_temp.prune_node_vary((final_X_train, final_y_train), (final_X_test, final_y_test), (final_X_valid, final_y_valid))\nplt.plot(d, a, label=\"training accuracy\")\nplt.plot(d, b, label=\"test accuracy\")\nplt.plot(d, c, label=\"validation accuracy\")\nplt.legend()\nplt.xlabel('Number of Nodes in the Decision Tree After Pruning')\nplt.ylabel('Accuracy')\nax = plt.gca()\nax.set_ylim([0, 1])\nplt.title(\"Evaluation Criterion - KMeans Clustering\")\nplt.savefig(f'accuracy_analysis.png', dpi=300, bbox_inches='tight')\nplt.clf()","metadata":{"id":"8Rfthvf5dUxk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"************************\nPruning Step : 0\nNumber of nodes in the tree: 3443\n************************\n\n************************\nPruning Step : 1\nNumber of nodes in the tree: 3325\n************************\n\n************************\nPruning Step : 2\nNumber of nodes in the tree: 3319\n************************\n\n************************\nPruning Step : 3\nNumber of nodes in the tree: 3289\n************************\n\n************************\nPruning Step : 4\nNumber of nodes in the tree: 3273\n************************\n\n************************\nPruning Step : 5\nNumber of nodes in the tree: 3268\n************************\n\n************************\nPruning Step : 6\nNumber of nodes in the tree: 3228\n************************\n\n************************\nPruning Step : 7\nNumber of nodes in the tree: 3190\n************************\n\n************************\nPruning Step : 8\nNumber of nodes in the tree: 3168\n************************\n\n************************\nPruning Step : 9\nNumber of nodes in the tree: 3156\n************************\n\n************************\nPruning Step : 10\nNumber of nodes in the tree: 3134\n************************\n\n************************\nPruning Step : 11\nNumber of nodes in the tree: 3118\n************************\n\n************************\nPruning Step : 12\nNumber of nodes in the tree: 3106\n************************\n\n************************\nPruning Step : 13\nNumber of nodes in the tree: 3104\n************************\n\n************************\nPruning Step : 14\nNumber of nodes in the tree: 3088\n************************\n\n************************\nPruning Step : 15\nNumber of nodes in the tree: 3056\n************************\n\n************************\nPruning Step : 16\nNumber of nodes in the tree: 3054\n************************","metadata":{}},{"cell_type":"code","source":"# If pruning takes too long, stop the execution of the cell and make use of the saved numpy variables to complete the analysis\na = np.load(open(filepath+\"train_accuracies.npy\", \"rb\"), allow_pickle=True)\nb = np.load(open(filepath+\"test_accuracies.npy\", \"rb\"), allow_pickle=True)\nc = np.load(open(filepath+\"val_accuracies.npy\", \"rb\"), allow_pickle=True)\nd = np.load(open(filepath+\"prunes.npy\", \"rb\"), allow_pickle=True)\nplt.plot(d, a, label=\"training accuracy\")\nplt.plot(d, b, label=\"test accuracy\")\nplt.plot(d, c, label=\"validation accuracy\")\nplt.legend()\nplt.xlabel('Number of Nodes in the Decision Tree After Pruning')\nplt.ylabel('Accuracy')\nax = plt.gca()\nax.set_ylim([0, 1])\nplt.title(\"Evaluation Criterion - KMeans Clustering\")\nplt.savefig(f'accuracy_analysis.png', dpi=300, bbox_inches='tight')\nplt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Accuracy Analysis during Pruning](https://github.com/Hritaban02/Decision-Tree-Classifier---A-New-Look---Using-Kmeans-Clustering-Inertia-as-a-Evaluation-Metric/blob/master/Pruning/accuracy_analysis.png)","metadata":{}},{"cell_type":"markdown","source":"# Post-Pruning Analysis","metadata":{"id":"ZNwzZmM_Z2Mp"}},{"cell_type":"code","source":"pruned_model = pickle.load(open(filepath+\"pruning_step_15_decision_tree.dat\", \"rb\"))\nfinal_X_train = np.load(open(filepath+\"final_X_train.npy\", \"rb\"), allow_pickle=True)\nfinal_y_train = np.load(open(filepath+\"final_y_train.npy\", \"rb\"), allow_pickle=True)\nfinal_X_test = np.load(open(filepath+\"final_X_test.npy\", \"rb\"), allow_pickle=True)\nfinal_y_test = np.load(open(filepath+\"final_y_test.npy\", \"rb\"), allow_pickle=True)\nfinal_X_valid = np.load(open(filepath+\"final_X_valid.npy\", \"rb\"), allow_pickle=True)\nfinal_y_valid = np.load(open(filepath+\"final_y_valid.npy\", \"rb\"), allow_pickle=True)\n\npostPruningAnalysisFile = open('postPruningAnalysisFile.txt', 'w')\nprint(\"Number of nodes in the pruned tree: \", len(pruned_model.node_dict), file=postPruningAnalysisFile)\n\ny_predict = pruned_model.predict(final_X_test)\nac = accuracy_score(final_y_test, y_predict)\nprint(f\"Test Accuracy after Pruning = {ac * 100}%\", file=postPruningAnalysisFile)\n\nclasses = np.unique(final_y_test)\nmatrix = confusion_matrix(final_y_test, y_predict, labels=classes)\nprint('Confusion matrix : \\n', matrix, file=postPruningAnalysisFile)\n\nmatrix = classification_report(final_y_test, y_predict, labels=classes)\nprint('Classification report : \\n', matrix, file=postPruningAnalysisFile)\npostPruningAnalysisFile.close()","metadata":{"id":"V-KPlgVgZzK4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Number of nodes in the pruned tree:  3054\n\n    Test Accuracy after Pruning = 89.26689027311932%\n\n    Confusion matrix : \n    \n     [[1564    0    1    0    7  140   26   15    0    1    0    0]\n     [   0    2    0    0    0    0    0    0    0    0    0    0]\n     [   1    0   34    0    0    0    0    0    0    0    0    0]\n     [   0    0    0  131   14    0    0    0    0    0    0    0]\n     [   9    0    0    8  418    7    0    1    0    0    8    0]\n     [ 100    0    0    0   10  639    4    4    0    0    0    0]\n     [  12    0    0    0    9   18  138    6    0    0    0    0]\n     [  17    0    1    0    3    8    0  184    0    0    0    0]\n     [   0    0    0    0    0    0    0    0  323    0    0    0]\n     [   0    0    0    0    0    0    0    0    0   15    0    0]\n     [   6    0    0    0    4    0    0    0    0    0  174    6]\n     [   0    0    0    0    0    0    0    0    0    0    2  104]]\n     \n    Classification report : \n               precision    recall  f1-score   support\n\n           A       0.92      0.89      0.90      1754\n           B       1.00      1.00      1.00         2\n           C       0.94      0.97      0.96        35\n           D       0.94      0.90      0.92       145\n           E       0.90      0.93      0.91       451\n           F       0.79      0.84      0.81       757\n           G       0.82      0.75      0.79       183\n           H       0.88      0.86      0.87       213\n           I       1.00      1.00      1.00       323\n           W       0.94      1.00      0.97        15\n           X       0.95      0.92      0.93       190\n           Y       0.95      0.98      0.96       106\n\n        accuracy                           0.89      4174\n       macro avg       0.92      0.92      0.92      4174\n    weighted avg       0.89      0.89      0.89      4174\n\n","metadata":{}},{"cell_type":"markdown","source":"# Analysis using n-Decision Attribute Combination","metadata":{"id":"NiPezJtCwM-H"}},{"cell_type":"code","source":"def n_decision_attribute_analysis(X_train, y_train, X_test, y_test, column_names, n):\n    model = DecisionTreeClassifier(reuse_attribute=True, max_depth=100, number_of_decision_attributes=n, distance_metric=None, clustering_algorithm='kmeans++', clustering_max_iter=300)\n    model.fit(X_train, y_train, column_names)\n    pickle.dump(model, open(f\"{n}_decision_attribute_decision_tree.dat\", \"wb\"))\n    model.print_decision_tree(f\"{n}_decision_attribute_\") \n    y_predict = model.predict(X_test)\n    ac = accuracy_score(y_test, y_predict)\n    return ac","metadata":{"id":"lHTzoWIpweVM","execution":{"iopub.status.busy":"2022-05-07T17:48:00.511810Z","iopub.execute_input":"2022-05-07T17:48:00.512630Z","iopub.status.idle":"2022-05-07T17:48:00.519844Z","shell.execute_reply.started":"2022-05-07T17:48:00.512590Z","shell.execute_reply":"2022-05-07T17:48:00.519231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"final_X_train = np.load(open(filepath+\"final_X_train.npy\", \"rb\"), allow_pickle=True)\nfinal_y_train = np.load(open(filepath+\"final_y_train.npy\", \"rb\"), allow_pickle=True)\nfinal_X_test = np.load(open(filepath+\"final_X_test.npy\", \"rb\"), allow_pickle=True)\nfinal_y_test = np.load(open(filepath+\"final_y_test.npy\", \"rb\"), allow_pickle=True)\nN = len(column_names)-1\nprint(N)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:48:02.754687Z","iopub.execute_input":"2022-05-07T17:48:02.755146Z","iopub.status.idle":"2022-05-07T17:48:02.823753Z","shell.execute_reply.started":"2022-05-07T17:48:02.755103Z","shell.execute_reply":"2022-05-07T17:48:02.822677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### n = 2","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 2)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'w')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 2 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 3","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 3)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 3 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 4","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 4)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 4 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 5","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 5)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 5 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 6","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 6)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 6 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 7","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 7)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 7 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 8","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 8)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 8 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:48:08.073702Z","iopub.execute_input":"2022-05-07T17:48:08.074010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n = 9","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 9)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 9 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***********************\nNumber of decision attributes = 9 \nAccuracy = 66.81839961667465%\n***********************","metadata":{}},{"cell_type":"markdown","source":"### n = 10","metadata":{}},{"cell_type":"code","source":"ac = n_decision_attribute_analysis(final_X_train, final_y_train, final_X_test, final_y_test, column_names, 10)\nnDecisionAttributeAnalysisFile = open('nDecisionAttributeAnalysisFile.txt', 'a')\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nprint(f\"Number of decision attributes = 10 \", file=nDecisionAttributeAnalysisFile)\nprint(f\"Accuracy = {ac * 100}%\", file=nDecisionAttributeAnalysisFile)\nprint(\"***********************\", file=nDecisionAttributeAnalysisFile)\nnDecisionAttributeAnalysisFile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***********************\nNumber of decision attributes = 10 \nAccuracy = 69.76521322472449%\n***********************\n","metadata":{}},{"cell_type":"markdown","source":"# Analysis PART - 2","metadata":{}},{"cell_type":"markdown","source":"### Let us perform a quick Exploratory Data Analysis on the Dataset and try to build an even better model using the same Decision Tree Classifier Model as above.","metadata":{}},{"cell_type":"code","source":"filepath = '../input/avila-dataset-and-saved-variables/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(filepath+'avila_combined.txt', delimiter=\",\", header=None)\n\ncolumn_names = ['index', 'intercolumnar distance', 'upper margin', 'lower margin', 'exploitation', 'row number', 'modular ratio',\n              'interlinear spacing', 'weight', 'peak number', 'modular ratio/ interlinear spacing']\n\ndf.isnull().sum()\n\nX = df.iloc[:, 0:-1].to_numpy()\ny = df.iloc[:, -1].to_numpy()\n\nindex = np.arange(X.shape[0])\nX = np.c_[index, X]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y)\nplt.legend()\nplt.xlabel('Class Label')\nplt.ylabel('Number of instances')\nplt.title(\"Exploratory Data Analysis\")\nplt.savefig(f'class_frequency.png', dpi=300, bbox_inches='tight')\nplt.clf()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It can be clearly seen that class 'A' dominates the dataset causing the decision tree to be extremely biased towards 'A'.\n\n### **Possible Approach: Build a Decision Tree to classify 'A' and NOT-'A' and then further classify NOT-'A' using another Decision Tree as one of the rest of the classes.**","metadata":{}},{"cell_type":"code","source":"X_sub, X_test, y_sub, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_valid, y_train, y_valid = train_test_split(X_sub, y_sub, test_size=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_modified = copy.deepcopy(y_train)\ny_train_modified[y_train != 'A'] = 'S'\n\ny_valid_modified = copy.deepcopy(y_valid)\ny_valid_modified[y_valid != 'A'] = 'S'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = DecisionTreeClassifier(reuse_attribute=False, max_depth=100, number_of_decision_attributes=1, distance_metric=None, clustering_algorithm='kmeans++', clustering_max_iter=300)\nmodel1.fit(X_train, y_train_modified, column_names)\npickle.dump(model1, open(f\"model1_decision_tree.dat\", \"wb\"))\nmodel1.print_decision_tree(f\"model1_\")\n\nnum_nodes = len(model1.node_dict)\nmodel1.prune((X_valid, y_valid_modified), minimum_increase=0.000001, maximum_rounds=num_nodes, sample_size=num_nodes, stopping_rounds=0)\npickle.dump(model1, open(f\"pruned_model1_decision_tree.dat\", \"wb\"))\nmodel1.print_decision_tree(f\"pruned_model1_\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"************************\nPruning Step:  0\nNumber of nodes in the Tree:  411\nAccuracy:  0.6880239520958084\n************************\n************************\nPruning Step:  1\nNumber of nodes in the Tree:  397\nAccuracy:  0.6904191616766467\n************************\n************************\nPruning Step:  2\nNumber of nodes in the Tree:  373\nAccuracy:  0.692814371257485\n************************\n************************\nPruning Step:  3\nNumber of nodes in the Tree:  371\nAccuracy:  0.6946107784431138\n************************\n************************\nPruning Step:  4\nNumber of nodes in the Tree:  369\nAccuracy:  0.6958083832335329\n************************\n************************\nPruning Step:  5\nNumber of nodes in the Tree:  367\nAccuracy:  0.6970059880239521\n************************\n************************\nPruning Step:  6\nNumber of nodes in the Tree:  361\nAccuracy:  0.6976047904191617\n************************\n************************\nPruning Step:  7\nNumber of nodes in the Tree:  359\nAccuracy:  0.6982035928143713\n************************\n************************\nPruning Step:  8\nNumber of nodes in the Tree:  327\nAccuracy:  0.6988023952095809\n************************\n************************\nPruning Step:  9\nNumber of nodes in the Tree:  321\nAccuracy:  0.6994011976047905\n************************","metadata":{}},{"cell_type":"code","source":"model2 = DecisionTreeClassifier(reuse_attribute=False, max_depth=100, number_of_decision_attributes=1, distance_metric=None, clustering_algorithm='kmeans++', clustering_max_iter=300)\nmodel2.fit(X_train[y_train != 'A'], y_train[y_train != 'A'], column_names)\npickle.dump(model2, open(f\"model2_decision_tree.dat\", \"wb\"))\nmodel2.print_decision_tree(f\"model2_\")\n\nnum_nodes = len(model2.node_dict)\nmodel2.prune((X_valid[y_valid != 'A'], y_valid[y_valid != 'A']), minimum_increase=0.000001, maximum_rounds=num_nodes, sample_size=num_nodes, stopping_rounds=0)\npickle.dump(model2, open(f\"pruned_model2_decision_tree.dat\", \"wb\"))\nmodel2.print_decision_tree(f\"pruned_model2_\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"************************\nPruning Step:  0\nNumber of nodes in the Tree:  1537\nAccuracy:  0.9242579324462641\n************************\n************************\nPruning Step:  1\nNumber of nodes in the Tree:  1529\nAccuracy:  0.9263050153531218\n************************\n************************\nPruning Step:  2\nNumber of nodes in the Tree:  1523\nAccuracy:  0.9283520982599796\n************************\n************************\nPruning Step:  3\nNumber of nodes in the Tree:  1509\nAccuracy:  0.9293756397134084\n************************\n************************\nPruning Step:  4\nNumber of nodes in the Tree:  1501\nAccuracy:  0.9303991811668373\n************************\n************************\nPruning Step:  5\nNumber of nodes in the Tree:  1498\nAccuracy:  0.9314227226202662\n************************\n************************\nPruning Step:  6\nNumber of nodes in the Tree:  1474\nAccuracy:  0.932446264073695\n************************\n************************\nPruning Step:  7\nNumber of nodes in the Tree:  1472\nAccuracy:  0.9334698055271239\n************************\n************************\nPruning Step:  8\nNumber of nodes in the Tree:  1465\nAccuracy:  0.9344933469805528\n************************\n************************\nPruning Step:  9\nNumber of nodes in the Tree:  1463\nAccuracy:  0.9355168884339816\n************************\n************************\nPruning Step:  10\nNumber of nodes in the Tree:  1461\nAccuracy:  0.9365404298874105\n************************\n************************\nPruning Step:  11\nNumber of nodes in the Tree:  1459\nAccuracy:  0.9375639713408394\n************************\n************************\nPruning Step:  12\nNumber of nodes in the Tree:  1437\nAccuracy:  0.9385875127942682\n************************\n************************\nPruning Step:  13\nNumber of nodes in the Tree:  1435\nAccuracy:  0.9396110542476971\n************************\n************************\nPruning Step:  14\nNumber of nodes in the Tree:  1433\nAccuracy:  0.9406345957011258\n************************\n************************\nPruning Step:  15\nNumber of nodes in the Tree:  1431\nAccuracy:  0.9416581371545547\n************************\n************************\nPruning Step:  16\nNumber of nodes in the Tree:  1418\nAccuracy:  0.9426816786079836\n************************\n************************\nPruning Step:  17\nNumber of nodes in the Tree:  1416\nAccuracy:  0.9437052200614124\n************************","metadata":{}},{"cell_type":"code","source":"def predict_using_model1_and_model2(X):\n    model1_y_predict = np.asarray(model1.predict(X))\n    model1_y_predict[model1_y_predict == 'S'] = np.asarray(model2.predict(X[model1_y_predict == 'S']))\n    return model1_y_predict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = predict_using_model1_and_model2(X_test)\nac = accuracy_score(y_test, y_predict)\nprint(f\"Accuracy using model1 and model2= {ac * 100}%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy using model1 and model2= 63.34451365596549%","metadata":{}},{"cell_type":"markdown","source":"### **We see that this approach does not work better rather performs much worse. A plausible reason can be that in our approach we depend on similarity amongst data points to cluster them and then perform the node split. Thus in model1, at any node which is to be split it is likely that the split is not optimal because 'S' is actually a mixture of 'B', 'C' etc.**\n\n### Example: An example of 'A' can be close to 'B', but not that close. In this second approach, the model1 may see that the example is close to 'S' (because it is close to 'B')  and thus sends it to model2, but model2 has no way to classify it as 'A' thus leading to a missclassification.\n\n### This clearly shows that model1 bottlenecks model2.\n\n### ***But the age old question remains, can we do better?***","metadata":{}}]}